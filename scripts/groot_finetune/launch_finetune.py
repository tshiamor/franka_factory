"""
Launch GR00T N1.6 fine-tuning for Franka MCX Card Block Insertion.

This is our own launch script that wraps Isaac-GR00T's fine-tuning pipeline
with memory optimizations for RTX 5090 (32GB), without modifying any
Isaac-GR00T source files.

Usage:
    cd /home/tshiamo/Isaac-GR00T
    export PATH="/home/tshiamo/miniforge3/envs/isaaclab/bin:$PATH"
    export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    python /home/tshiamo/SIMULATION_MANIPULATION/franka_factory/scripts/groot_finetune/launch_finetune.py
"""

import os
import importlib
import sys
from pathlib import Path

import tyro

from gr00t.configs.base_config import get_default_config
from gr00t.configs.finetune_config import FinetuneConfig
from gr00t.experiment.experiment import run


def load_modality_config(modality_config_path: str):
    path = Path(modality_config_path)
    if path.exists() and path.suffix == ".py":
        sys.path.append(str(path.parent))
        importlib.import_module(path.stem)
        print(f"Loaded modality config: {path}")
    else:
        raise FileNotFoundError(f"Modality config path does not exist: {modality_config_path}")


if __name__ == "__main__":
    if "LOGURU_LEVEL" not in os.environ:
        os.environ["LOGURU_LEVEL"] = "INFO"

    ft_config = tyro.cli(FinetuneConfig, description=__doc__)
    embodiment_tag = ft_config.embodiment_tag.value

    if ft_config.modality_config_path is not None:
        load_modality_config(ft_config.modality_config_path)

    config = get_default_config().load_dict(
        {
            "data": {
                "download_cache": False,
                "datasets": [
                    {
                        "dataset_paths": [ft_config.dataset_path],
                        "mix_ratio": 1.0,
                        "embodiment_tag": embodiment_tag,
                    }
                ],
            }
        }
    )
    config.load_config_path = None

    # Model config (same as Isaac-GR00T's launch_finetune.py)
    config.model.tune_llm = ft_config.tune_llm
    config.model.tune_visual = ft_config.tune_visual
    config.model.tune_projector = ft_config.tune_projector
    config.model.tune_diffusion_model = ft_config.tune_diffusion_model
    config.model.state_dropout_prob = ft_config.state_dropout_prob
    config.model.random_rotation_angle = ft_config.random_rotation_angle
    config.model.color_jitter_params = ft_config.color_jitter_params

    config.model.load_bf16 = False
    config.model.reproject_vision = False
    config.model.eagle_collator = True
    config.model.model_name = "nvidia/Eagle-Block2A-2B-v2"
    config.model.backbone_trainable_params_fp32 = True
    config.model.use_relative_action = True

    # Training config (same as Isaac-GR00T's launch_finetune.py)
    config.training.start_from_checkpoint = ft_config.base_model_path
    config.training.global_batch_size = ft_config.global_batch_size
    config.training.dataloader_num_workers = ft_config.dataloader_num_workers
    config.training.learning_rate = ft_config.learning_rate
    config.training.gradient_accumulation_steps = ft_config.gradient_accumulation_steps
    config.training.output_dir = ft_config.output_dir
    config.training.save_steps = ft_config.save_steps
    config.training.save_total_limit = ft_config.save_total_limit
    config.training.num_gpus = ft_config.num_gpus
    config.training.use_wandb = ft_config.use_wandb
    config.training.max_steps = ft_config.max_steps
    config.training.weight_decay = ft_config.weight_decay
    config.training.warmup_ratio = ft_config.warmup_ratio
    config.training.wandb_project = "finetune-gr00t-n1d6"

    # --- RTX 5090 (32GB) memory optimizations ---
    # These are the only lines that differ from Isaac-GR00T's launch_finetune.py
    config.training.optim = "adamw_bnb_8bit"       # 8-bit optimizer (saves ~75% optimizer VRAM)
    config.training.gradient_checkpointing = True   # trades compute for ~50% less activation VRAM

    # Data config
    config.data.shard_size = ft_config.shard_size
    config.data.episode_sampling_rate = ft_config.episode_sampling_rate
    config.data.num_shards_per_epoch = ft_config.num_shards_per_epoch

    run(config)
